\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=1.5cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}

\usepackage{listings}
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures

\usepackage{graphicx} % Required for the inclusion of images
\graphicspath{{./images/}} % Specifies the directory where pictures are stored

\setlength{\droptitle}{-6em}

\begin{document}

\center
Aprendizagem 2024\\
Homework I -- Group 016\\
(ist1106022, ist1106720)\vskip 1cm

\large{\textbf{Part I}: Pen and paper}\normalsize

\begin{enumerate}[leftmargin=\labelsep, label=\textbf{\arabic*.)}]
    \item \begin{itemize}
              \item Data Subset for \( y_1 \geq 0.3 \) \\
                    \vspace{0.5em}
                    \begin{minipage}{0.30\textwidth}
                        \begin{itemize}
                            \item Class A $\rightarrow 3 \ (x_7, x_8, x_{11})$
                            \item Class B $\rightarrow 2 \ (x_6, x_{12})$
                            \item Class C $\rightarrow 2 \ (x_9, x_{10})$
                        \end{itemize}
                    \end{minipage}
                    \begin{minipage}{0.45\textwidth}
                        \begin{center}
                            \[\begin{array}{|c|c|c|c|c|c|}
                                    \hline
                                    D      & y_1  & y_2 & y_3 & y_4 & y_{\text{out}} \\
                                    \hline
                                    x_6    & 0.30 & 0   & 1   & 0   & B              \\
                                    x_7    & 0.76 & 0   & 1   & 1   & A              \\
                                    x_8    & 0.86 & 1   & 0   & 0   & A              \\
                                    x_9    & 0.93 & 0   & 1   & 1   & C              \\
                                    x_{10} & 0.47 & 0   & 1   & 1   & C              \\
                                    x_{11} & 0.73 & 1   & 0   & 0   & A              \\
                                    x_{12} & 0.89 & 1   & 2   & 0   & B              \\
                                    \hline
                                \end{array}\]
                        \end{center}
                    \end{minipage}

              \item Entropy Calculation for \( H(y_{\text{out}}) \)

                    \[
                        H(y_{\text{out}}) = -\frac{3}{7} \log_2 \frac{3}{7} - \frac{2}{7} \log_2 \frac{2}{7} - \frac{2}{7} \log_2 \frac{2}{7} = 1.557
                    \]

              \item Calculating \( H(y_{\text{out}}|y_2) \)

                    \[
                        H(y_{\text{out}}|y_2) = \frac{4}{7} \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{2} \log_2 \frac{1}{2}\right) + \frac{3}{7} \times \left( -\frac{2}{3} \log_2 \frac{2}{3} - \frac{1}{3} \log_2 \frac{1}{3} \right) = 1.251
                    \]

              \item Information Gain for \( y_2 \)

                    \[
                        IG(y_2) = H(y_{\text{out}}) - H(y_{\text{out}}|y_2) = 1.557 - 1.251 = 0.306
                    \]

              \item Calculating \( H(y_{\text{out}}|y_3) \)

                    \[
                        H(y_{\text{out}}|y_3) = \frac{2}{7} \times \left( -1 \log_2 1 \right) + \frac{4}{7} \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2} \right) + \frac{1}{7} \times \left( -1 \log_2 1 \right) = 0.857
                    \]

              \item Information Gain for \( y_3 \)

                    \[
                        IG(y_3) = H(y_{\text{out}}) - H(y_{\text{out}}|y_3) = 1.557 - 0.857 = 0.7
                    \]

              \item Calculating \( H(y_{\text{out}}|y_4) \)

                    \[
                        H(y_{\text{out}}|y_4) = \frac{4}{7} \times \left( -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2} \right) + \frac{3}{7} \times \left( -\frac{1}{3} \log_2 \frac{1}{3} - \frac{2}{3} \log_2 \frac{2}{3} \right) = 0.965
                    \]

              \item Information Gain for \( y_4 \)

                    \[
                        IG(y_4) = H(y_{\text{out}}) - H(y_{\text{out}}|y_4) = 1.557 - 0.965 = 0.592
                    \]

              \item Information Gain Evaluation

                    Since feature \( y_3 \) has the greatest information gain, it is selected as the splitting criterion.

                    \begin{itemize}
                        \item \( y_3 = 0 \rightarrow \text{Class A} \)
                        \item \( y_3 = 1 \rightarrow \text{Class A/B/C} \)
                        \item \( y_3 = 2 \rightarrow \text{Class B} \)
                    \end{itemize}

              \item Data Subset for \( y_1 \geq 0.3 \) and \( y_3 = 1 \) \\
                    \vspace{0.5em}
                    \begin{minipage}{0.30\textwidth}
                        \begin{itemize}
                            \item Class A $\rightarrow 1 \ (x_7)$
                            \item Class B $\rightarrow 1 \ (x_6)$
                            \item Class C $\rightarrow 2 \ (x_9, x_{10})$
                        \end{itemize}
                    \end{minipage}
                    \begin{minipage}{0.45\textwidth}
                        \[
                            \begin{array}{|c|c|c|c|c|c|}
                                \hline
                                D      & y_1  & y_2 & y_3 & y_4 & y_{\text{out}} \\
                                \hline
                                x_6    & 0.30 & 0   & 1   & 0   & B              \\
                                x_7    & 0.76 & 0   & 1   & 1   & A              \\
                                x_9    & 0.93 & 0   & 1   & 1   & C              \\
                                x_{10} & 0.47 & 0   & 1   & 1   & C              \\
                                \hline
                            \end{array}
                        \]
                    \end{minipage}

              \item Entropy Calculation for \( H(y_{\text{out}}) \)

                    \[
                        H(y_{\text{out}}) = -\frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2} = 1.500
                    \]

              \item Calculating \( H(y_{\text{out}}|y_2) \)

                    \[
                        H(y_{\text{out}}|y_2) = 1 \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{2} \log_2 \frac{1}{2}\right) = 1.500
                    \]

              \item Information Gain for \( y_2 \)

                    \[
                        IG(y_2) = H(y_{\text{out}}) - H(y_{\text{out}}|y_2) = 1.500 - 1.500 = 0
                    \]

              \item Calculating \( H(y_{\text{out}}|y_4) \)

                    \[
                        H(y_{\text{out}}|y_4) = \frac{1}{4} \times \left( - 1 \log_2 1 \right) + \frac{3}{4} \times \left( -\frac{1}{3} \log_2 \frac{1}{3} - \frac{2}{3} \log_2 \frac{2}{3} \right) = 0.689
                    \]

              \item Information Gain for \( y_4 \)

                    \[
                        IG(y_4) = H(y_{\text{out}}) - H(y_{\text{out}}|y_4) = 1.500 - 0.689 = 0.811
                    \]

              \item Information Gain Evaluation

                    Since feature \( y_4 \) has the greatest information gain, it is selected as the splitting criterion.

                    \begin{itemize}
                        \item \( y_4 = 0 \rightarrow \text{Class B} \)
                        \item \( y_3 = 1 \rightarrow \text{Class A/C} \)
                    \end{itemize}

              \item Decision Tree

                    Since it is not possible to create any more subsets with a minimum of 4 observations, we can build the tree, taking into account that any ties are resolved by the majority class. \\

                    % \begin{center}
                    %    \includegraphics[width=0.8\textwidth]{decision_tree_pen_and_paper.jpg}
                    %    \captionof{figure}{Decision tree drawn for the pen and paper exercise 1}
                    %\end{center}
          \end{itemize}
    \item \begin{itemize}
              \item Calculating Confusion Matrix entries

                    $[\text{predicted}, \text{true}] = \text{value}$
                    \begin{itemize}
                        \item $[A, A] = 2$ \hspace{1cm} $[A, B] = 0$ \hspace{1cm} $[A, C] = 0$
                        \item $[B, A] = 0$ \hspace{1cm} $[B, B] = 2 + 1 + 1 = 4$ \hspace{1cm} $[B, C] = 0$
                        \item $[C, A] = 1$ \hspace{1cm} $[C, B] = 0$ \hspace{1cm} $[C, C] = 1 + 2 + 2 = 5$
                    \end{itemize}
              \item Filling in the Matrix
                    \[
                        \begin{array}{|c|c|c|c|}
                            \hline
                            \text{Predicted \textbackslash True} & A & B & C \\
                            \hline
                            A                                    & 2 & 0 & 0 \\
                            \hline
                            B                                    & 0 & 4 & 0 \\
                            \hline
                            C                                    & 1 & 0 & 5 \\
                            \hline
                        \end{array}
                    \]
          \end{itemize}
    \item \begin{itemize}
              \item Class A Calculations

                    \[
                        \text{Precision}_A = \frac{TP}{TP + FP} = \frac{2}{2 + 0} = 1.0
                    \]

                    \[
                        \text{Recall}_A = \frac{TP}{TP + FN} = \frac{2}{2 + 1} = \frac{2}{3}
                    \]

                    \[
                        F1_A = 2 \times \frac{\text{Precision}_A \times \text{Recall}_A}{\text{Precision}_A + \text{Recall}_A} = 2 \times \frac{1.0 \times \frac{2}{3}}{1.0 + \frac{2}{3}} = \frac{4}{5}
                    \]

              \item Class B Calculations

                    \[
                        \text{Precision}_B = \frac{TP}{TP + FP} = \frac{4}{4 + 0} = 1.0
                    \]

                    \[
                        \text{Recall}_B = \frac{TP}{TP + FN} = \frac{4}{4 + 0} = 1.0
                    \]

                    \[
                        F1_B = 2 \times \frac{\text{Precision}_B \times \text{Recall}_B}{\text{Precision}_B + \text{Recall}_B} = 2 \times \frac{1.0 \times 1.0}{1.0 + 1.0} = 1.0
                    \]

              \item Class C Calculations

                    \[
                        \text{Precision}_C = \frac{TP}{TP + FP} = \frac{5}{5 + 1} = \frac{5}{6}
                    \]

                    \[
                        \text{Recall}_C = \frac{TP}{TP + FN} = \frac{5}{5 + 0} = 1.0
                    \]

                    \[
                        F1_C = 2 \times \frac{\text{Precision}_C \times \text{Recall}_C}{\text{Precision}_C + \text{Recall}_C} = 2 \times \frac{\frac{5}{6} \times 1.0}{\frac{5}{6} + 1.0} = \frac{10}{11}
                    \]

              \item Conclusion

                    The Class with the lowest training F1 score is \textbf{Class A}.
          \end{itemize}
    \item \begin{itemize}
              \item Bin Creation

                    \begin{itemize}
                        \item Bin 1: $[0, 0.2[$
                        \item Bin 2: $[0.2, 0.4[$
                        \item Bin 3: $[0.4, 0.6[$
                        \item Bin 4: $[0.6, 0.8[$
                        \item Bin 5: $[0.8, 1]$
                    \end{itemize}

              \item Placement of $y_1$ values in respective bins

                    \begin{itemize}
                        \item Bin 1: Class A = 0, Class B = 1 (0.06), Class C = 2 (0.16, 0.01)
                        \item Bin 2: Class A = 0, Class B = 2 (0.21, 0.30), Class C = 1 (0.22)
                        \item Bin 3: Class A = 0 , Class B = 0, Class C = 1 (0.47)
                        \item Bin 4: Class A = 2 (0.73, 0.76), Class B = 0, Class C = 0
                        \item Bin 5: Class A = 1 (0.86), Class B = 1 (0.89), Class C = 1 (0.93)
                    \end{itemize}

              \item Relative Frequencies Calculation

                    \[
                        \begin{array}{|c|c|c|c|}
                            \hline
                            \text{Bin}   & \text{Class A} & \text{Class B} & \text{Class C} \\
                            \hline
                            \text{Bin 1} & 0              & 0.25           & 0.40           \\
                            \text{Bin 2} & 0              & 0.50           & 0.20           \\
                            \text{Bin 3} & 0              & 0              & 0.20           \\
                            \text{Bin 4} & 0.667          & 0              & 0              \\
                            \text{Bin 5} & 0.333          & 0.25           & 0.20           \\
                            \hline
                        \end{array}
                    \]

              \item Histogram

              \item $n$-ary Root Split Calculation

                    To find the $n$-ary root split, the dominant class must be singled out for each bin:

                    \begin{itemize}
                        \item Bin 1: Dominant Class C (0.40)
                        \item Bin 2: Dominant Class B (0.50)
                        \item Bin 3: Dominant Class C (0.20)
                        \item Bin 4: Dominant Class A (0.667)
                        \item Bin 5: Class A is also present (0.333)
                    \end{itemize}

              \item Conclusion

                    The \( n \)-ary root split can be defined based on the dominant classes:

                    \begin{itemize}
                        \item The first split could occur at Bin 1 with Class C.
                        \item The second split could occur at Bin 2 with Class B.
                        \item Subsequent splits would be based on the observed distributions in the remaining bins, particularly focusing on Class A’s dominance in Bin 4.
                    \end{itemize}

                    The decision tree's splits will optimize for maximum information gain based on these empirical distributions.

          \end{itemize}
\end{enumerate}

\large{\textbf{Part II}: Programming}\normalsize

\begin{enumerate}[leftmargin=\labelsep, label=\textbf{\arabic*.)}]
    \item To compare the performance of a kNN classifier with $k=5$ and a naive Bayes classifier, a 5-fold stratified cross-validation was performed on the on a heart disease dataset:

        \begin{enumerate}[label=\textbf{\alph*.)}]
            \item aa
            \item bb
            \item cc 
        \end{enumerate}

    \item Empty

    \item Empty 

    \item Empty 
    
\end{enumerate}
\end{document}
