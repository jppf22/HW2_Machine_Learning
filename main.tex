\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=1.5cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}

\usepackage{listings}
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures

\usepackage{graphicx} % Required for the inclusion of images
\graphicspath{{./images/}} % Specifies the directory where pictures are stored

\setlength{\droptitle}{-6em}

\begin{document}

\center
Aprendizagem 2024\\
Homework I -- Group 016\\
(ist1106022, ist1106720)\vskip 1cm

\large{\textbf{Part I}: Pen and paper}\normalsize

\begin{enumerate}[leftmargin=\labelsep, label=\textbf{\arabic*.)}]
    \item \begin{itemize}
              \item Data Subset for \( y_1 \geq 0.3 \) \\
                    \vspace{0.5em}
                    \begin{minipage}{0.30\textwidth}
                        \begin{itemize}
                            \item Class A $\rightarrow 3 \ (x_7, x_8, x_{11})$
                            \item Class B $\rightarrow 2 \ (x_6, x_{12})$
                            \item Class C $\rightarrow 2 \ (x_9, x_{10})$
                        \end{itemize}
                    \end{minipage}
                    \begin{minipage}{0.45\textwidth}
                        \begin{center}
                            \[\begin{array}{|c|c|c|c|c|c|}
                                    \hline
                                    D      & y_1  & y_2 & y_3 & y_4 & y_{\text{out}} \\
                                    \hline
                                    x_6    & 0.30 & 0   & 1   & 0   & B              \\
                                    x_7    & 0.76 & 0   & 1   & 1   & A              \\
                                    x_8    & 0.86 & 1   & 0   & 0   & A              \\
                                    x_9    & 0.93 & 0   & 1   & 1   & C              \\
                                    x_{10} & 0.47 & 0   & 1   & 1   & C              \\
                                    x_{11} & 0.73 & 1   & 0   & 0   & A              \\
                                    x_{12} & 0.89 & 1   & 2   & 0   & B              \\
                                    \hline
                                \end{array}\]
                        \end{center}
                    \end{minipage}

              \item Entropy Calculation for \( H(y_{\text{out}}) \)

                    \[
                        H(y_{\text{out}}) = -\frac{3}{7} \log_2 \frac{3}{7} - \frac{2}{7} \log_2 \frac{2}{7} - \frac{2}{7} \log_2 \frac{2}{7} = 1.557
                    \]

              \item Calculating \( H(y_{\text{out}}|y_2) \)

                    \[
                        H(y_{\text{out}}|y_2) = \frac{4}{7} \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{2} \log_2 \frac{1}{2}\right) + \frac{3}{7} \times \left( -\frac{2}{3} \log_2 \frac{2}{3} - \frac{1}{3} \log_2 \frac{1}{3} \right) = 1.251
                    \]

              \item Information Gain for \( y_2 \)

                    \[
                        IG(y_2) = H(y_{\text{out}}) - H(y_{\text{out}}|y_2) = 1.557 - 1.251 = 0.306
                    \]

              \item Calculating \( H(y_{\text{out}}|y_3) \)

                    \[
                        H(y_{\text{out}}|y_3) = \frac{2}{7} \times \left( -1 \log_2 1 \right) + \frac{4}{7} \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2} \right) + \frac{1}{7} \times \left( -1 \log_2 1 \right) = 0.857
                    \]

              \item Information Gain for \( y_3 \)

                    \[
                        IG(y_3) = H(y_{\text{out}}) - H(y_{\text{out}}|y_3) = 1.557 - 0.857 = 0.7
                    \]

              \item Calculating \( H(y_{\text{out}}|y_4) \)

                    \[
                        H(y_{\text{out}}|y_4) = \frac{4}{7} \times \left( -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2} \right) + \frac{3}{7} \times \left( -\frac{1}{3} \log_2 \frac{1}{3} - \frac{2}{3} \log_2 \frac{2}{3} \right) = 0.965
                    \]

              \item Information Gain for \( y_4 \)

                    \[
                        IG(y_4) = H(y_{\text{out}}) - H(y_{\text{out}}|y_4) = 1.557 - 0.965 = 0.592
                    \]

              \item Information Gain Evaluation

                    Since feature \( y_3 \) has the greatest information gain, it is selected as the splitting criterion.

                    \begin{itemize}
                        \item \( y_3 = 0 \rightarrow \text{Class A} \)
                        \item \( y_3 = 1 \rightarrow \text{Class A/B/C} \)
                        \item \( y_3 = 2 \rightarrow \text{Class B} \)
                    \end{itemize}

              \item Data Subset for \( y_1 \geq 0.3 \) and \( y_3 = 1 \) \\
                    \vspace{0.5em}
                    \begin{minipage}{0.30\textwidth}
                        \begin{itemize}
                            \item Class A $\rightarrow 1 \ (x_7)$
                            \item Class B $\rightarrow 1 \ (x_6)$
                            \item Class C $\rightarrow 2 \ (x_9, x_{10})$
                        \end{itemize}
                    \end{minipage}
                    \begin{minipage}{0.45\textwidth}
                        \[
                            \begin{array}{|c|c|c|c|c|c|}
                                \hline
                                D      & y_1  & y_2 & y_3 & y_4 & y_{\text{out}} \\
                                \hline
                                x_6    & 0.30 & 0   & 1   & 0   & B              \\
                                x_7    & 0.76 & 0   & 1   & 1   & A              \\
                                x_9    & 0.93 & 0   & 1   & 1   & C              \\
                                x_{10} & 0.47 & 0   & 1   & 1   & C              \\
                                \hline
                            \end{array}
                        \]
                    \end{minipage}

              \item Entropy Calculation for \( H(y_{\text{out}}) \)

                    \[
                        H(y_{\text{out}}) = -\frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2} = 1.500
                    \]

              \item Calculating \( H(y_{\text{out}}|y_2) \)

                    \[
                        H(y_{\text{out}}|y_2) = 1 \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{2} \log_2 \frac{1}{2}\right) = 1.500
                    \]

              \item Information Gain for \( y_2 \)

                    \[
                        IG(y_2) = H(y_{\text{out}}) - H(y_{\text{out}}|y_2) = 1.500 - 1.500 = 0
                    \]

              \item Calculating \( H(y_{\text{out}}|y_4) \)

                    \[
                        H(y_{\text{out}}|y_4) = \frac{1}{4} \times \left( - 1 \log_2 1 \right) + \frac{3}{4} \times \left( -\frac{1}{3} \log_2 \frac{1}{3} - \frac{2}{3} \log_2 \frac{2}{3} \right) = 0.689
                    \]

              \item Information Gain for \( y_4 \)

                    \[
                        IG(y_4) = H(y_{\text{out}}) - H(y_{\text{out}}|y_4) = 1.500 - 0.689 = 0.811
                    \]

              \item Information Gain Evaluation

                    Since feature \( y_4 \) has the greatest information gain, it is selected as the splitting criterion.

                    \begin{itemize}
                        \item \( y_4 = 0 \rightarrow \text{Class B} \)
                        \item \( y_3 = 1 \rightarrow \text{Class A/C} \)
                    \end{itemize}

              \item Decision Tree

                    Since it is not possible to create any more subsets with a minimum of 4 observations, we can build the tree, taking into account that any ties are resolved by the majority class. \\
                    \begin{center}
                        \includegraphics[width=0.8\textwidth]{decision_tree_pen_and_paper.jpg}
                        \captionof{figure}{Decision tree drawn for the pen and paper exercise 1}
                    \end{center}
          \end{itemize}
    \item \begin{itemize}
              \item Calculating Confusion Matrix entries

                    $[\text{predicted}, \text{true}] = \text{value}$
                    \begin{itemize}
                        \item $[A, A] = 2$ \hspace{1cm} $[A, B] = 0$ \hspace{1cm} $[A, C] = 0$
                        \item $[B, A] = 0$ \hspace{1cm} $[B, B] = 2 + 1 + 1 = 4$ \hspace{1cm} $[B, C] = 0$
                        \item $[C, A] = 1$ \hspace{1cm} $[C, B] = 0$ \hspace{1cm} $[C, C] = 1 + 2 + 2 = 5$
                    \end{itemize}
              \item Filling in the Matrix
                    \[
                        \begin{array}{|c|c|c|c|}
                            \hline
                            \text{Predicted \textbackslash True} & A & B & C \\
                            \hline
                            A                                    & 2 & 0 & 0 \\
                            \hline
                            B                                    & 0 & 4 & 0 \\
                            \hline
                            C                                    & 1 & 0 & 5 \\
                            \hline
                        \end{array}
                    \]
          \end{itemize}
    \item \begin{itemize}
              \item Class A Calculations

                    \[
                        \text{Precision}_A = \frac{TP}{TP + FP} = \frac{2}{2 + 0} = 1.0
                    \]

                    \[
                        \text{Recall}_A = \frac{TP}{TP + FN} = \frac{2}{2 + 1} = \frac{2}{3}
                    \]

                    \[
                        F1_A = 2 \times \frac{\text{Precision}_A \times \text{Recall}_A}{\text{Precision}_A + \text{Recall}_A} = 2 \times \frac{1.0 \times \frac{2}{3}}{1.0 + \frac{2}{3}} = \frac{4}{5}
                    \]

              \item Class B Calculations

                    \[
                        \text{Precision}_B = \frac{TP}{TP + FP} = \frac{4}{4 + 0} = 1.0
                    \]

                    \[
                        \text{Recall}_B = \frac{TP}{TP + FN} = \frac{4}{4 + 0} = 1.0
                    \]

                    \[
                        F1_B = 2 \times \frac{\text{Precision}_B \times \text{Recall}_B}{\text{Precision}_B + \text{Recall}_B} = 2 \times \frac{1.0 \times 1.0}{1.0 + 1.0} = 1.0
                    \]

              \item Class C Calculations

                    \[
                        \text{Precision}_C = \frac{TP}{TP + FP} = \frac{5}{5 + 1} = \frac{5}{6}
                    \]

                    \[
                        \text{Recall}_C = \frac{TP}{TP + FN} = \frac{5}{5 + 0} = 1.0
                    \]

                    \[
                        F1_C = 2 \times \frac{\text{Precision}_C \times \text{Recall}_C}{\text{Precision}_C + \text{Recall}_C} = 2 \times \frac{\frac{5}{6} \times 1.0}{\frac{5}{6} + 1.0} = \frac{10}{11}
                    \]

              \item Conclusion

                    The Class with the lowest training F1 score is \textbf{Class A}.
          \end{itemize}
    \item \begin{itemize}
              \item Bin Creation

                    \begin{itemize}
                        \item Bin 1: $[0, 0.2[$
                        \item Bin 2: $[0.2, 0.4[$
                        \item Bin 3: $[0.4, 0.6[$
                        \item Bin 4: $[0.6, 0.8[$
                        \item Bin 5: $[0.8, 1]$
                    \end{itemize}

              \item Placement of $y_1$ values in respective bins

                    \begin{itemize}
                        \item Bin 1: Class A = 0, Class B = 1 (0.06), Class C = 2 (0.16, 0.01)
                        \item Bin 2: Class A = 0, Class B = 2 (0.21, 0.30), Class C = 1 (0.22)
                        \item Bin 3: Class A = 0 , Class B = 0, Class C = 1 (0.47)
                        \item Bin 4: Class A = 2 (0.73, 0.76), Class B = 0, Class C = 0
                        \item Bin 5: Class A = 1 (0.86), Class B = 1 (0.89), Class C = 1 (0.93)
                    \end{itemize}

              \item Relative Frequencies Calculation

                    \[
                        \begin{array}{|c|c|c|c|}
                            \hline
                            \text{Bin}   & \text{Class A} & \text{Class B} & \text{Class C} \\
                            \hline
                            \text{Bin 1} & 0              & 0.25           & 0.40           \\
                            \text{Bin 2} & 0              & 0.50           & 0.20           \\
                            \text{Bin 3} & 0              & 0              & 0.20           \\
                            \text{Bin 4} & 0.667          & 0              & 0              \\
                            \text{Bin 5} & 0.333          & 0.25           & 0.20           \\
                            \hline
                        \end{array}
                    \]

              \item Histogram

                    \begin{center}
                        \includegraphics[width=0.8\textwidth]{bin_histogram.jpg}
                        \captionof{figure}{Histogram of the relative frequencies of the classes in each bin}
                    \end{center}

              \item $n$-ary Root Split Calculation

                    To find the $n$-ary root split, the dominant class must be singled out for each bin:

                    \begin{itemize}
                        \item Bin 1: Dominant Class C (0.40)
                        \item Bin 2: Dominant Class B (0.50)
                        \item Bin 3: Dominant Class C (0.20)
                        \item Bin 4: Dominant Class A (0.667)
                        \item Bin 5: Class A is also present (0.333)
                    \end{itemize}

              \item Conclusion

                    The \( n \)-ary root split can be defined based on the dominant classes:

                    \begin{itemize}
                        \item The first split could occur at Bin 1 with Class C.
                        \item The second split could occur at Bin 2 with Class B.
                        \item Subsequent splits would be based on the observed distributions in the remaining bins, particularly focusing on Class A’s dominance in Bin 4.
                    \end{itemize}

                    The decision tree's splits will optimize for maximum information gain based on these empirical distributions.

          \end{itemize}
\end{enumerate}

\large{\textbf{Part II}: Programming}\normalsize

\begin{enumerate}[leftmargin=\labelsep, label=\textbf{\arabic*.)}]
    \item Applying \texttt{f\_classif} from the \texttt{sklearn} library upon the dataset (after splitting into feature data matrix and target vector) allows understanding of the discrimantive power of each feature:

          \begin{lstlisting}[basicstyle=\ttfamily\centering]
'Pregnancies': 39.67
'Glucose': 213.16
'BloodPressure': 3.26
'SkinThickness': 4.3
'Insulin': 13.28
'BMI': 71.77
'DiabetesPedigreeFunction': 23.87
'Age': 46.14
        \end{lstlisting}
          The scores indicate that \texttt{Glucose} is the \textbf{most discriminative} feature, whereas \texttt{BloodPressure} is the \textbf{least discriminative}.

          The following plot shows the class-condition probability density functions for these two features:

          \begin{center}
              \includegraphics{discrimantive_features_dense_probability.png}
              \captionof{figure}{Class-condition probability density functions for the most and least discriminative features}
          \end{center}

    \item To measure accuracy levels originated from using a decision tree with the minimums sample split values (2, 5, 10, 20, 30, 50, 100) when branching, a stratified 80-20 training-testing split was performed.

          \begin{center}
              \includegraphics{split_training_results.png}
              \captionof{figure}{Accuracy levels for different minimum sample splits}
          \end{center}

    \item The smaller decision tree minimum splits have the highest accuracy levels on the training set, while having the smaller values on the testing set, which indicates overfitting and poor generalization capabilities for these levels.

          The higher minimum splits lead to increansigly better accuracy values on the testing set, although continuing to decrease the accuracy on the training set, which indicates the model is starting to have better generalization capabilities and less overfitting.

          Out of all the available minimum splits, the best pick would be the one stricking the best balance between overall accuracy and variance), maximizing the first while minimizing the second, which in this case would be 100.

          Ideally with higher minimum sample split values, there would be a value leading high and identical accuracy levels on both sets. However, there could also be underfitted values.
    \item .
          \begin{enumerate}[label=\textbf{\roman*.)},leftmargin=0em]
              \item Using \textit{all} data to train a single decision tree classifier with a maximum depth of 3 leads to the following tree):\\
                    \begin{center}
                        \includegraphics{decision_tree_value_counts.png}
                        \captionof{figure}{Decision tree with values and samples expressed in amount}
                    \end{center}

                    But with this second view of the same tree, we can more easily understand the probability of each class in each node (\texttt{value} and \texttt{samples} expressed in proportion):

                    \begin{center}
                        \includegraphics{decision_tree.png}
                        \captionof{figure}{Decision tree color-coded by class probability and with values and samples expressed in proportion}
                    \end{center}

                    The \textcolor{orange}{"oranger"} the node, the higher the probability of the class being 0 (eq. non-diabetic), and the \textcolor{blue}{"bluer"} the node, the higher the probability of the class being 1 (eq. diabetic).

              \item Overall, high \texttt{Glucose} and \texttt{BMI} are the most common indicator of diabetes (agreeing with our analysis in \textit{exercise 1}).

                    Following is a thorough analysis of the probability in each conditional association (extracted from the graph):

                    \begin{itemize}
                        \item $P( \text{\texttt{Diabetes}}) = 0.349$
                    \end{itemize}
                    \begin{itemize}
                        \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$}}) = 0.194$:
                              \begin{itemize}
                                  \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $\leq 28.5$}}) = 0.085$:
                                        \begin{itemize}
                                            \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $\leq 28.5$ \& BMI $\leq 45.4$}}) = 0.075$
                                            \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $\leq 28.5$ \& BMI $> 45.4$}}) = 0.75$
                                        \end{itemize}
                                  \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $> 28.5$}}) = 0.332$:
                                        \begin{itemize}
                                            \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $> 28.5$ \& BMI $\leq 26.35$}}) = 0.049$
                                            \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $> 28.5$ \& BMI $> 26.35$}}) = 0.399$
                                        \end{itemize}
                              \end{itemize}
                    \end{itemize}
                    \begin{itemize}
                        \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$}}) = 0.615$:
                              \begin{itemize}
                                  \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $\leq 29.95$}}) = 0.316$:
                                        \begin{itemize}
                                            \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $\leq 29.95$ \& Glucose $\leq 145.5$}}) = 0.146$
                                            \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $\leq 29.95$ \& Glucose $> 145.5$}}) = 0.514$
                                        \end{itemize}
                                  \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $> 29.95$}}) = 0.725$:
                                        \begin{itemize}
                                            \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $> 29.95$ \& Glucose $\leq 157.5$}}) = 0.609$
                                            \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $> 29.95$ \& Glucose $> 157.5$}}) = 0.87$
                                        \end{itemize}
                              \end{itemize}
                    \end{itemize}

                    From the analysis it is possible to easily understand the likelihood of a new patient, given his retrieved medical data, to have diabetes or not.

          \end{enumerate}
\end{enumerate}
\end{document}
